# Part 4: 한계와 위험 관리 (Risks & Ethics)

이 파트에서는 생성형 AI의 한계와 윤리적 고려사항을 학습합니다.

---

## 📖 이 파트에서 배우는 것

| 챕터 | 주제 | 핵심 질문 |
|------|------|----------|
| Ch.9 | 환각의 이해와 대응 | AI는 왜 거짓말을 하는가? |
| Ch.10 | 윤리적 사용과 책임 | 어디까지가 허용되는가? |
| Ch.11 | 책임 있는 AI 사용 | 어떻게 책임감 있게 사용하는가? |

---

## 🎯 학습 목표

이 파트를 마치면 다음을 할 수 있습니다:

- [x] 환각(Hallucination)의 발생 원인과 유형을 이해한다
- [x] AI 결과물의 팩트체킹 프로세스를 수행할 수 있다
- [x] 표절과 학습 보조의 경계를 판단할 수 있다
- [x] AI 사용 시 윤리적 원칙을 적용할 수 있다

---

## ⚠️ 이 파트가 중요한 이유

!!! danger "무지한 사용의 위험"
    
    생성형 AI를 제대로 이해하지 않고 사용하면:
    
    - **학술적 위험**: 조작된 인용, 표절 문제
    - **법적 위험**: 저작권 침해, 명예훼손
    - **비즈니스 위험**: 잘못된 정보 기반 의사결정
    - **개인적 위험**: 프라이버시 침해

---

## 📑 챕터 목록

### Chapter 9: 환각(Hallucination)의 이해와 대응
- 9.1 환각이 발생하는 메커니즘
- 9.2 환각 유형: 사실 날조 vs 논리적 오류
- 9.3 탐지 방법과 팩트체킹 프로세스
- 📌 사례: Mata v. Avianca, Google Bard 사건

[:octicons-arrow-right-24: Chapter 9 읽기](ch09-hallucination.md)

### Chapter 10: 윤리적 사용과 책임
- 10.1 표절과 학습 보조의 경계
- 10.2 AI 생성물의 출처 표기법
- 10.3 편향성 인식과 비판적 평가
- 10.4 데이터 프라이버시와 보안 원칙

[:octicons-arrow-right-24: Chapter 10 읽기](ch10-ethics.md)

### Chapter 11: 책임 있는 AI 사용 원칙
- 11.1 인간이 최종 판단자: AI는 도구다
- 11.2 투명성과 책임의 귀속
- 11.3 나만의 AI 윤리 강령 수립

[:octicons-arrow-right-24: Chapter 11 읽기](ch11-responsible-use.md)
