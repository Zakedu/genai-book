# 컨텍스트 윈도우 (Context Window)

!!! info "최종 수정일: 2025-01-16"

**컨텍스트 윈도우(Context Window)**는 AI가 한 번에 처리할 수 있는 토큰의 최대 개수로, AI 활용의 한계와 가능성을 결정짓는 핵심 개념입니다.

---

## 정의

컨텍스트 윈도우는 LLM이 한 번의 대화에서 **"기억"하고 처리할 수 있는 텍스트의 최대 길이**입니다. 이 창(window) 안에 들어온 정보만 AI가 인식할 수 있으며, 창 밖의 정보는 존재하지 않는 것과 같습니다.

!!! tip "핵심 비유"
    컨텍스트 윈도우는 **"AI의 작업 메모리"**와 같습니다. 책상 위에 올려놓을 수 있는 종이의 양이 제한되어 있는 것처럼, AI도 한 번에 처리할 수 있는 정보량에 한계가 있습니다.

---

## 컨텍스트 윈도우의 구성

### 입력과 출력의 합산

컨텍스트 윈도우는 **입력 토큰 + 출력 토큰**을 합산하여 계산됩니다.

```
컨텍스트 윈도우 = 시스템 프롬프트 + 사용자 입력 + AI 응답 + 대화 히스토리
```

| 구성 요소 | 설명 | 예시 |
|----------|------|------|
| **시스템 프롬프트** | AI의 역할과 행동 지침 | "당신은 친절한 비서입니다..." |
| **대화 히스토리** | 이전 대화 내용 | 이전 질문과 답변들 |
| **현재 입력** | 사용자의 현재 질문/요청 | "이 문서를 요약해줘" + 문서 내용 |
| **AI 응답** | AI가 생성하는 답변 | 생성될 요약 내용 |

### 실제 계산 예시

```
예시: 128K 토큰 컨텍스트 윈도우

시스템 프롬프트:     500 토큰
대화 히스토리:    20,000 토큰
현재 입력:       50,000 토큰
--------------------------
사용 가능한 출력: 57,500 토큰

총계: 128,000 토큰
```

!!! warning "주의사항"
    대화가 길어질수록 히스토리가 쌓여 **새로운 입력과 출력에 사용할 수 있는 공간이 줄어듭니다**. 이것이 긴 대화에서 AI가 이전 내용을 "잊어버리는" 것처럼 보이는 이유입니다.

---

## 주요 모델별 컨텍스트 윈도우

### 2025년 1월 기준 현황

| 모델 | 컨텍스트 윈도우 | 대략적 분량 | 특징 |
|------|----------------|-------------|------|
| **GPT-4 Turbo** | 128K 토큰 | 영어 약 96,000단어 | OpenAI 주력 모델 |
| **GPT-4o** | 128K 토큰 | 영어 약 96,000단어 | 멀티모달 지원 |
| **Claude 3.5 Sonnet** | 200K 토큰 | 영어 약 150,000단어 | 긴 문서 처리에 강점 |
| **Claude 3 Opus** | 200K 토큰 | 영어 약 150,000단어 | 최고 성능 모델 |
| **Gemini 1.5 Pro** | 1M~2M 토큰 | 영어 약 750,000~1,500,000단어 | 최대 컨텍스트 |
| **Llama 3** | 8K~128K 토큰 | 버전에 따라 상이 | 오픈소스 |

### 분량 환산 가이드

| 토큰 수 | 영어 분량 | 한국어 분량 | 실제 예시 |
|--------|----------|------------|----------|
| 4K | 약 3,000단어 | 약 1,500자 | 짧은 블로그 글 |
| 32K | 약 24,000단어 | 약 12,000자 | 중편 소설 1편 |
| 128K | 약 96,000단어 | 약 48,000자 | 책 1권 분량 |
| 200K | 약 150,000단어 | 약 75,000자 | 두꺼운 책 1권 |
| 1M | 약 750,000단어 | 약 375,000자 | 백과사전 수준 |

!!! info "한국어와 토큰"
    한국어는 영어보다 같은 의미를 전달하는 데 **약 1.5~2배 더 많은 토큰**이 필요합니다. 따라서 실제 한국어 처리 가능 분량은 영어보다 적습니다.

---

## 컨텍스트 윈도우의 중요성

### 1. 대화 연속성

| 상황 | 결과 |
|------|------|
| 컨텍스트 내 | AI가 이전 대화 내용을 기억하고 일관된 응답 제공 |
| 컨텍스트 초과 | 이전 대화 내용이 잘리며, AI가 맥락을 잃음 |

### 2. 문서 처리 능력

```
긴 문서 처리 시나리오:

문서 크기: 100,000 토큰
모델 A (32K 컨텍스트): ❌ 처리 불가 - 문서가 너무 큼
모델 B (128K 컨텍스트): ✅ 처리 가능 - 여유 있음
모델 C (200K 컨텍스트): ✅ 처리 가능 - 추가 지시 여유도 있음
```

### 3. 복잡한 작업 수행

| 작업 유형 | 필요 컨텍스트 | 설명 |
|----------|-------------|------|
| 간단한 Q&A | 4K~8K | 짧은 질문과 답변 |
| 문서 요약 | 32K~128K | 긴 문서 전체를 읽고 요약 |
| 코드 리팩토링 | 32K~128K | 여러 파일의 코드 이해 |
| 책 전체 분석 | 128K~1M | 책 한 권을 통째로 분석 |
| RAG 시스템 | 32K~128K | 검색 결과 + 질문 + 응답 |

---

## 컨텍스트 관리 전략

### 1. 효율적인 토큰 사용

**불필요한 내용 제거:**

| 비효율적 | 효율적 |
|:---------|:-------|
| 장황한 배경 설명 | 핵심 정보만 제공 |
| 전체 문서 입력 | 관련 부분만 추출 |
| 반복되는 지시사항 | 시스템 프롬프트로 통합 |

### 2. 청킹(Chunking) 전략

긴 문서를 처리할 때 분할하는 방법입니다.

```
전략 1: 순차적 처리
[문서 1/4] → 요약1 → [문서 2/4] → 요약2 → ... → 최종 통합

전략 2: 계층적 요약
[전체 문서] → [섹션별 요약] → [최종 요약]

전략 3: 질문 기반 추출
[전체 문서] → [관련 부분만 추출] → [답변 생성]
```

### 3. 대화 히스토리 관리

```python
# 개념적 예시
def manage_context(conversation_history, max_tokens=100000):
    """
    대화 히스토리가 너무 길어지면 오래된 내용 제거
    """
    current_tokens = count_tokens(conversation_history)

    if current_tokens > max_tokens * 0.8:  # 80% 도달 시
        # 방법 1: 오래된 메시지 삭제
        # 방법 2: 이전 대화 요약으로 압축
        # 방법 3: 중요한 정보만 유지
        pass
```

---

## 컨텍스트 윈도우의 한계와 해결책

### "Lost in the Middle" 문제

!!! warning "중간 정보 손실 현상"
    연구에 따르면, 매우 긴 컨텍스트에서 AI는 **처음과 끝 부분의 정보는 잘 기억하지만, 중간 부분의 정보는 상대적으로 덜 정확하게 처리**하는 경향이 있습니다.[^1]

**대응 전략:**

| 전략 | 설명 |
|------|------|
| 중요 정보 배치 | 핵심 정보를 처음이나 끝에 배치 |
| 명시적 참조 | "위에서 언급한 X에 대해..." |
| 구조화 | 명확한 섹션 구분과 헤딩 사용 |
| 반복 강조 | 중요한 정보 여러 번 언급 |

### 컨텍스트 확장 기술

| 기술 | 설명 | 현재 상태 |
|------|------|----------|
| **Sparse Attention** | 모든 토큰이 아닌 중요 토큰만 참조 | 연구 진행 중 |
| **RAG** | 외부 지식 검색으로 컨텍스트 보완 | 실용화 단계 |
| **메모리 시스템** | 장기 기억 별도 저장 | 초기 단계 |
| **압축 기술** | 컨텍스트를 효율적으로 압축 | 연구 진행 중 |

---

## 실전 활용 팁

### 모델 선택 가이드

| 작업 | 권장 컨텍스트 | 추천 모델 |
|------|-------------|----------|
| 일상적 대화 | 8K~32K | GPT-4o mini, Claude 3 Haiku |
| 문서 분석 | 64K~128K | GPT-4 Turbo, Claude 3.5 Sonnet |
| 책/논문 전체 분석 | 128K~200K | Claude 3 Opus, GPT-4 Turbo |
| 대규모 코드베이스 | 200K+ | Gemini 1.5 Pro, Claude 3 |
| 영상 스크립트 분석 | 1M+ | Gemini 1.5 Pro |

### 비용 최적화

```
컨텍스트 크기 ↑ = 비용 ↑

최적화 방법:
1. 필요한 정보만 입력
2. 요약된 형태로 제공
3. 작은 모델로 전처리 후 큰 모델 사용
4. 캐싱 활용 (일부 API 지원)
```

---

## 미래 전망

### 컨텍스트 윈도우의 확장 추세

| 연도 | 대표 모델 | 최대 컨텍스트 |
|------|----------|--------------|
| 2022 | GPT-3.5 | 4K 토큰 |
| 2023 | GPT-4 | 32K 토큰 |
| 2024 | Claude 3, Gemini 1.5 | 200K~2M 토큰 |
| 2025+ | 차세대 모델 | 10M+ 토큰 예상 |

!!! tip "트렌드"
    컨텍스트 윈도우는 매년 급격히 확장되고 있으며, 이는 AI가 처리할 수 있는 정보의 범위가 계속 넓어지고 있음을 의미합니다. 궁극적으로는 "무한 컨텍스트"를 향해 발전 중입니다.

---

## 관련 문서

- [토큰](token.md) - 컨텍스트 윈도우의 기본 단위
- [프롬프트](prompt.md) - 컨텍스트 내 효과적인 입력 방법
- [Chapter 1: 생성형 AI란](../../part1/ch01-what-is-genai.md) - AI 기본 개념
- [부록: 도구 비교표](../../appendix/tools-comparison.md) - 모델별 상세 비교

---

## 참고 자료

[^1]: Liu, N., et al. (2023). "Lost in the Middle: How Language Models Use Long Contexts." *arXiv preprint arXiv:2307.03172*.

1. Anthropic. (2024). "Claude's Context Window." https://docs.anthropic.com/
2. OpenAI. (2024). "GPT-4 Turbo with 128K Context." https://platform.openai.com/docs/
3. Google. (2024). "Gemini 1.5 Pro: A Million Token Context Window." https://blog.google/technology/ai/google-gemini-next-generation-model/

