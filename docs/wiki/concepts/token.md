# 토큰 (Token)

!!! info "최종 수정일: 2025-01-16"

**토큰(Token)**은 AI가 텍스트를 처리하는 기본 단위로, AI 사용 비용과 성능을 이해하는 데 핵심적인 개념입니다.

---

## 정의

토큰은 AI 모델이 텍스트를 이해하고 생성할 때 사용하는 **최소 처리 단위**입니다. 단어, 단어의 일부, 문장 부호, 또는 특수 문자가 각각 토큰이 될 수 있습니다.

!!! tip "핵심 개념"
    AI는 텍스트를 글자나 단어 단위가 아닌 **토큰 단위**로 처리합니다. "토크나이저(Tokenizer)"라는 도구가 텍스트를 토큰으로 분해하고, 모델은 이 토큰들을 숫자(ID)로 변환하여 처리합니다.

---

## 토큰화의 원리

### 토크나이저의 작동 방식

```
입력 텍스트: "ChatGPT는 훌륭한 AI입니다"
     ↓
토크나이저 처리
     ↓
토큰 분해: ["Chat", "G", "PT", "는", " 훌륭", "한", " AI", "입니다"]
     ↓
토큰 ID 변환: [19731, 38, 2898, 168, 31619, 168, 9552, 46695]
     ↓
모델 처리
```

### 주요 토크나이저 알고리즘

| 알고리즘 | 사용 모델 | 특징 |
|----------|----------|------|
| **BPE** (Byte Pair Encoding) | GPT 시리즈 | 빈도 기반 서브워드 분해 |
| **WordPiece** | BERT, Gemini | 가능성 기반 분해 |
| **SentencePiece** | Claude, LLaMA | 언어 독립적 처리 |

---

## 언어별 토큰 효율성

### 영어 vs 한국어

토큰화 효율성은 언어에 따라 크게 다릅니다.

| 언어 | 1,000자 당 토큰 수 | 1토큰 당 글자 수 | 효율성 |
|------|-------------------|-----------------|--------|
| **영어** | 약 250 토큰 | 약 4글자 | 높음 |
| **한국어** | 약 500-700 토큰 | 약 1.5-2글자 | 낮음 |
| **중국어** | 약 300-400 토큰 | 약 2.5-3글자 | 중간 |
| **일본어** | 약 400-500 토큰 | 약 2글자 | 중간 |

!!! warning "한국어 사용자 주의"
    한국어는 영어 대비 2-3배 더 많은 토큰을 소비합니다. 이는 API 비용이 2-3배 더 높아질 수 있음을 의미합니다.

### 실제 토큰 계산 예시

**영어 예시:**
```
"Hello, how are you today?"
토큰: ["Hello", ",", " how", " are", " you", " today", "?"]
토큰 수: 7개
```

**한국어 예시:**
```
"안녕하세요, 오늘 기분이 어떠세요?"
토큰: ["안녕", "하세요", ",", " 오늘", " 기분", "이", " 어떠", "세요", "?"]
토큰 수: 9-12개 (토크나이저에 따라 다름)
```

---

## 토큰이 중요한 이유

### 1. API 비용 계산

AI API 서비스는 토큰 수를 기준으로 요금을 부과합니다.

**GPT-4o 기준 예시 (2025년 1월 기준):**

| 토큰 유형 | 가격 (1M 토큰) | 한국어 1,000자 비용 |
|----------|---------------|-------------------|
| 입력 토큰 | $2.50 | 약 $0.00125-0.00175 |
| 출력 토큰 | $10.00 | 약 $0.005-0.007 |

**월간 사용량 예시:**
```
- 일일 프롬프트: 평균 500 토큰 × 50회 = 25,000 토큰
- 일일 응답: 평균 1,000 토큰 × 50회 = 50,000 토큰
- 월간 총 토큰: (25,000 + 50,000) × 30 = 2,250,000 토큰
- 예상 월 비용: 약 $5-10 (모델에 따라 다름)
```

### 2. 컨텍스트 윈도우 제한

각 모델은 한 번에 처리할 수 있는 최대 토큰 수가 정해져 있습니다.

| 모델 | 컨텍스트 윈도우 | 대략적 분량 |
|------|----------------|-------------|
| GPT-4o | 128K 토큰 | 영어 책 약 300페이지 |
| Claude 3.5 Sonnet | 200K 토큰 | 영어 책 약 500페이지 |
| Gemini 1.5 Pro | 2M 토큰 | 영어 책 약 5,000페이지 |

!!! info "컨텍스트 윈도우란?"
    모델이 한 번에 "기억"할 수 있는 텍스트의 최대 길이입니다. 입력(프롬프트)과 출력(응답)의 토큰을 합한 값이 이 제한을 초과할 수 없습니다.

### 3. 응답 길이 제어

출력 토큰 수를 제한하여 응답 길이를 조절할 수 있습니다.

```python
# OpenAI API 예시
response = openai.ChatCompletion.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "AI란 무엇인가요?"}],
    max_tokens=500  # 응답을 500 토큰으로 제한
)
```

---

## 토큰 계산 도구

### 온라인 토크나이저

각 AI 서비스는 토큰 수를 미리 확인할 수 있는 도구를 제공합니다.

| 서비스 | 도구 | URL |
|--------|------|-----|
| OpenAI | Tokenizer | https://platform.openai.com/tokenizer |
| Anthropic | Token Counter | API 내장 기능 |
| Google | AI Studio | https://aistudio.google.com |

### Python으로 토큰 계산

```python
# tiktoken 라이브러리 사용 (OpenAI 모델용)
import tiktoken

# GPT-4o용 인코더
encoder = tiktoken.encoding_for_model("gpt-4o")

text = "안녕하세요, 생성형 AI에 대해 알려주세요."
tokens = encoder.encode(text)

print(f"텍스트: {text}")
print(f"토큰 수: {len(tokens)}")
print(f"토큰 목록: {tokens}")
print(f"디코딩: {[encoder.decode([t]) for t in tokens]}")
```

---

## 토큰 효율화 전략

### 1. 프롬프트 최적화

불필요한 텍스트를 줄여 토큰을 절약합니다.

| 비효율적 프롬프트 | 효율적 프롬프트 | 절감률 |
|:----------------|:--------------|:-------|
| "안녕하세요. 저는 오늘 당신에게 한 가지 질문을 드리고 싶습니다. 혹시 괜찮으시다면 AI에 대해서 설명해 주실 수 있을까요?" | "AI란 무엇인지 간단히 설명해주세요." | 약 70% |

### 2. 시스템 프롬프트 재사용

반복적인 시스템 프롬프트는 한 번만 설정하고 재사용합니다.

### 3. 응답 길이 제한

필요한 만큼만 응답을 받도록 `max_tokens`를 적절히 설정합니다.

### 4. 요약 활용

긴 문서는 먼저 요약한 후 작업하면 토큰을 절약할 수 있습니다.

```
[비효율적] 10,000 토큰 문서 전체를 입력 → 분석 요청
[효율적] 10,000 토큰 문서 → 먼저 요약 (2,000 토큰) → 요약본으로 분석
```

---

## 토큰과 관련된 기술 개념

### 임베딩 (Embedding)

토큰은 내부적으로 고차원 벡터(숫자 배열)로 변환됩니다. 이 벡터를 "임베딩"이라고 합니다.

```
토큰 "AI" → [0.123, -0.456, 0.789, ...] (수천 차원의 벡터)
```

### 어텐션 (Attention)

토큰들 간의 관계를 계산하는 메커니즘입니다. 컨텍스트 윈도우 내의 모든 토큰 쌍에 대해 어텐션 계산이 이루어집니다.

### 토큰 확률 (Token Probability)

AI는 다음 토큰으로 가능한 모든 토큰의 확률을 계산하고, 그 중에서 선택합니다.

```
현재까지 생성: "오늘 날씨가 정말"
다음 토큰 후보:
- "좋" (확률 35%)
- "나쁘" (확률 20%)
- "덥" (확률 15%)
- "춥" (확률 10%)
- ... 기타
```

---

## 특수 토큰

AI 모델은 특수한 목적의 토큰을 사용합니다.

| 특수 토큰 | 용도 | 예시 |
|----------|------|------|
| `<|endoftext|>` | 텍스트 종료 표시 | GPT 시리즈 |
| `[CLS]`, `[SEP]` | 문장 구분 | BERT |
| `<s>`, `</s>` | 시퀀스 시작/종료 | LLaMA |
| `[INST]`, `[/INST]` | 지시문 표시 | Llama 2 Chat |

---

## 관련 문서

- [컨텍스트 윈도우](context-window.md) - 토큰 제한과 관련된 개념
- [Chapter 1: 생성형 AI란](../../part1/ch01-what-is-genai.md) - AI 작동 원리
- [부록: 도구 비교표](../../appendix/tools-comparison.md) - 모델별 컨텍스트 윈도우 비교

---

## 참고 자료

1. OpenAI. (2024). "Tokenizer Tool." https://platform.openai.com/tokenizer
2. Sennrich, R., et al. (2016). "Neural Machine Translation of Rare Words with Subword Units." *ACL 2016*.
3. Kudo, T., & Richardson, J. (2018). "SentencePiece: A simple and language independent subword tokenizer." *EMNLP 2018*.

